\documentclass[a4paper]{article}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{parskip}

\begin{document}
	
\section{Overview}
3i innovation for digital media
\begin{itemize}
	\item Immersive
	\item Interactive
	\item Intelligent
\end{itemize}


\section{Computer Graphics Pipeline}
Computer graphics: ``the method of using computer technology to convert data into visual representations''. 3D graphics uses 3d representation of geometric data stored in computers to perform calculations and render digital images.


3D graphic phases
\begin{itemize}
	\item Modelling, creating the shape of objects
	\item Animation, defining movements and positions of objects overtime
	\item Rendering, generate images/videos 
\end{itemize}

Computer graphics pipeline
\begin{itemize}
	\item 3d geometry modeling
	\item Motion syn
	\item rigging
	\item textures
	\item lighting
	\item rendering
\end{itemize}

\subsection{Geometric modeling}

Geometry
\begin{itemize}
	\item Explicit geometry: triangle meshes, patches, point clouds
	\item Implicit geometry: parametric surfaces, procedural generation
	\item Volumetric data: voxel grid by scientific scans
	\item Generated by: 3d scans, CAD, from images
\end{itemize}

Triangle meshes are the most common. Neural representation uses a neural network to map multiple 2d images into a 3d scene.

\subsection{Motion Synthesis}
Generated from
\begin{itemize}
	\item Motion capture, 
	\item Key frames
	\item Kinematics/simulation
	\item Scripting
\end{itemize}

\subsection{Rigging}
Allow geometry deformation by motion. Created by
\begin{itemize}
	\item weighted blending from skeletal motion
	\item physics
	\item real world data
\end{itemize}

\subsection{Material/Textures}
Material types
\begin{itemize}
	\item Color (diffuse/spectral)
	\item Reflectance model (BRDF)
	\item Geometry details (normal maps)
\end{itemize}

To map material onto 3d models
\begin{itemize}
	\item Texture mapping
	\item Environment mapping
\end{itemize}

To create materials
\begin{itemize}
	\item Scanning real world objects (cameras, light stage)
	\item Artist creation
	\item Procedural generation
\end{itemize}

\subsection{Lighting}
Lighting types
\begin{itemize}
	\item Point and spot lights
	\item Directional lights
	\item Area lights
	\item Environment lights (radiance map)
\end{itemize}

\subsection{Rendering}
Rendering is to produce the final images using computer graphic algorithms.

Inverse rendering is to derive scene information from a rendered image/video, which estimates: cameras, lights, materials, and geometry. This is used in computer vision.

\subsection{Composition}
Composition is the combining of multiple renders into one. Methods
\begin{itemize}
	\item Green screen, blue screen
	\item LED screens
\end{itemize}

\section{Display and Interaction}
Pixels
\begin{itemize}
	\item Images are stored in the pixel array that is displayed on the screen
	\item A frame buffer is this block of memory dedicated to containing the pixel array.
	\item A pixel is a basic element of a frame buffer
	\item Resolution is the dim of the frame buffer
	\item Color depth is the number of bits per pixel
	\item Alpha is a component for transparency
	\item Buffer size computed by mul resolution and color depth.
\end{itemize}

There are 2d displays (phones, computers), 3d displays (lasers, volumetric), immersive displays (VR, large screen).

Interaction
\begin{itemize}
	\item Key component for communication and control
	\item Physical interactions using hardware (physical, 3d sensing, haptic device)
	\item Logical interfaces are abstract input mechanisms implemented per application (string, locator, pick, choice, dials)
\end{itemize}

\subsection{Applications}
Lists of applications
\begin{itemize}
	\item Movie and films
	\item Computer games
	\item Visualization
	\item Simulation
	\item XR (VR/AR)
	\item Training
	\item Design
\end{itemize}

\section{3D Geometry}

\subsection{Explicit geometry}
Polygon Mesh
\begin{itemize}
	\item A mesh is a collection of triangles. Each mesh models an object.
	\item Each triangle is defined by 3 vectors in 3d under a coordinate system.
	\item Vertices are 3d points. Can include additional data like: color, normal, texture coordinate
	\item Edges are lines linking two vertices
	\item Faces are planar elements bounded by edges
	\item Polygons are planar closed shape made of straight edges
	\item Surfaces are continuous shape formed by a connected set of polygon faces
\end{itemize}

Triangles (why triangles)
\begin{itemize}
	\item Minimum number of points to form a surface
	\item Any polygon can be subdivided into triangles, including other triangles
	\item Always defines a unique plane. Four or more points may not lie on the same plane
	\item Efficient for rasteriation and rendering pipeline
	\item Easy interpolation using barycentric coordinates
\end{itemize}

Topology of polygon mesh
\begin{itemize}
	\item Mesh topology is how the vertices/faces are arranged in a mesh.
	\item Need to answer queries like: vertices adjacent to a given vertex, faces sharing the same vertex.
	\item Need to store mesh connectivity information
	\item Vertex-vertex representation, mapping every vertex to neighboring vertices. Simple, lightweight, does not explicitly store faces. Good for vertice neighboring queries. Not good for edge/face topology queries
	\item Face-vertex represetation, mapping every face to a list of vertices defining it. (Optionally can have the reverse mapping from vertices to faces). Used in mesh file formats (OBJ). Suitable for wide range of applications, efficient storage
\end{itemize}

Topology mesh data formats
\begin{itemize}
	\item OBJ
	\item FBX
	\item PLY
	\item Other prop ones
\end{itemize}

Cross platform asset sharing
\begin{itemize}
	\item Incompatiabilities in file formats, materials, animation, and scene structures.
	\item Solved using open USD file format
\end{itemize}

OBJ file format (by wavefront tech)
\begin{itemize}
	\item \# is a comment
	\item \verb|v x y z (w)| for vertex
	\item \verb|vt u (v) (w)| for vertex texture coord
	\item \verb|vn x y z| for vertex normal
	\item \verb|f v1 v2 ...| for faces to vertex index
	\item For faces, can include vt and vn by
	\verb|f v1/vt1/vn1 v2/vt2/vn2 ...|
	where both the vn and vt can be optional
	\item o for object names
	\item g for group names
	\item \verb|mtlib file.mtl| for current material
\end{itemize}

Points and vectors
\begin{itemize}
	\item Points (vertices) are positions in space
	\item Vectors (normals) represents directions
	\item Relationships between points and vectors are:
	\item Point sub point is vector
	\item Point add vector is point
	\item Vector add vector is vector
	\item Point add point is undefined
\end{itemize}

Translation of points and vectors
\begin{itemize}
	\item Rendering pipeline transforms vertices/normals/texcoords. 
	\item Transformation on points and vectors nets different results: Translating points moves it to a new position. Translating a vector doesn't affect its direction or magnitude
	\item Homogeneous coordinates allows unified transformation of both points and vectors (later)
\end{itemize}

Vector operations
\begin{itemize}
	\item Dot product, $v\cdot w = |v| |w| \cos \theta$. Zero if perp
	\item Cross product, produce vector perp to both vectors. Use det trick. Use the right handed rule to get direction: u at index, v at middle, thumb is cross.
	\item Normalized vector is a unit vector. To normalize, divide each component by its length
\end{itemize}

Coordinate system
\begin{itemize}
	\item Form L with index and thumb, thumb points to $+x$, index points to $+y$.
	\item Left hand coordinate system use left hand middle finger direction as $+z$. Right hand coordinate system use right hand middle finger direction as $+z$. Unity use left-hand coordinate system 
\end{itemize}

Vertex winding order
\begin{itemize}
	\item Triangle specified by three vertices in order
	\item Assume left-handed coordinate system. For right-handed, flip the rotation
	\item Face with clockwise vertex ordering is front
	\item Face with counter-clockwise vertex ordering is back
	\item Normal vector point outwards on the front face
	\item Culling will stop rendering back faces, which is fine since most objects are closed.
\end{itemize}

Normals
\begin{itemize}
	\item Surface normal $n$ at point $P$ is a vector perp to the tangent plane of the surface at $P$.
	\item Vertex normal at a vertex $v$ is the normalized sum of the surface normals of all faces that shares the vertex $v$. Vertex normal is used for smooth shading in Phong.
	\item The number of vertices and vertex normals may be different due to reuse
	\item Surface normal of a triangle computed by taking normalized cross product of edges $(B-A) \times (C-A)$ in counter clockwise order if right handed, or clockwise if left handed coordinate system (normal points outwards on front face)
	\item Vertex normal is computed by normalizing sum of surface normals
	\[
		n = \frac{\sum_i n_{f_i}}{|\sum_i n_{f_i}|}
	\]
\end{itemize}

\subsection{Implicit geoemtry}
Implicit geometry
\begin{itemize}
	\item Implicit form is an equation that holds for all points $(x,y,z)$ on geometry. Parametric form are equations that traces the points on geometry given free/parametric variables
	\item For sphere, implicit
	\[
		x^2+y^2+z^2 = r^2
	\]
	parametric
	\begin{align*}
		x &= r \sin \phi \cos \theta\\
		y &= -r\cos \phi\\
		z &= -r \sin \phi \sin \theta
	\end{align*}
\end{itemize}

Line and ray
\begin{itemize}
	\item Line is a line between two 3d points. Parametric form
	\[
		P(t) = P_1 + t(P_2 - P_1)
	\]
	for $t \in [0,1]$
	\item Ray is a line that starts at a point and extends infinitely in a direction. Parametric form
	\[
		R(t) = P_0 + t v
	\]
	for $t > 0$
\end{itemize}

Plane
\begin{itemize}
	\item Given point $P_0$ on plane and normal vector $n$. Points $P$ are on the plane if
	\[
		n \cdot (P - P_0) = 0
	\]
	Alternatively the implicit form
	\[
		ax + by + cz = d
	\]
	\item For ray-plane intersection, sub ray into plane equation, we get
	\[
		t = - \frac{n\cdot(P_r - P_p)}{n\cdot r}
	\]
	and if $n \cdot r = 0$ then they are parallel
\end{itemize}

\subsection{Volume data}
Volume data
\begin{itemize}
	\item Represents object by sampling its interior at points. Stored uniformally on a 3d grid, where samples do not explicitly represent geometry 
	\item Sampled points are randomly distributed in 3d space. Containing: scalars (temperature/density) or vectors (velocity/force)
	\item Obtained from scanning devices, simulations, or procedural meshes
\end{itemize}

Regular grid
\begin{itemize}
	\item array of points defined by $x,y,z$
	\item spacing can be uniform or non-uniform
	\item visualized as stacked 2d images
	\item voxel is the smallest unit volume grid, representing sampled value at its position on the grid
\end{itemize}

Volume rendering
\begin{itemize}
	\item Treat volume as continuous field and do ray casting or ray marching
	\item Cast rays through volume. Sample field at discrete intervals $d$ along ray. Use a transfer function to convert voxel scalar values to color and opacity
\end{itemize}


\section{Fundamentals of Raytracing}
Rendering
\begin{itemize}
	\item Convert 3d polygon meshes to an image with color
	\item Notable algorithms like: raycasting, phong shading, texture mapping, whitted raytracing, distributed raytracing
\end{itemize}

Rasterization vs Raytracing
\begin{itemize}
	\item Rasterization: common in interactive graphics accelerated by GPU; independent per object shading; local illumination without considering entire scene; iterate over objects
	\item Raytracing: common in photo-realistic rendering offline; global illumination using full scene; iterate over pixels using viewing rays; accurate shadows/reflections/refractions
\end{itemize}

OpenGL rasterization pipeline
\begin{itemize}
	\item Data stream
	\item Vertex shader
	\item Geometry shader
	\item Rasterization using textures
	\item Fragment shader
	\item Frame buffer (optionally output to textures)
\end{itemize}

Raytracing pipeline
\begin{itemize}
	\item Ray generation, cast ray from camera for each pixel to determine its value
	\item Ray traversal, if ray hits, spawn additional rays for shadow/reflection/refraction
	\item Shading, if object is lit, compute its color based on direct light sources
	\item Blending, accumulate colors from direct and recursive rays for final pixel color
\end{itemize}

Ray casting
\begin{itemize}
	\item Developed in 1968
	\item Raytracing without the recursive ray traversal
	\item Still widely used for volume rendering. Also revisited using Neural Radiance Fields
\end{itemize}

Recursive raytracing
\begin{itemize}
	\item Developed by Whitted in 1980
	\item Extends raycasting by introducing recursive rays on: reflection (mirrors), refraction (transparent), shadows (lights)
	\item Supports global illumination
	\item Basis for realism
\end{itemize}

\subsection{Ray Generation}
Ray generation
\begin{itemize}
	\item Generate a ray from eye $P_0$ to point $P_1$ on an image plane
	\item Simply formula
	\[
		R(t) = P_0 + (P_1 - P_0)t
	\]
\end{itemize}

\subsection{Ray Intersection}
Ray Intersection
\begin{itemize}
	\item Need to find the first/closest surface point in scene geometry that intersects a ray.
	\item Ray-plane intersection: done in last chapter
	\item Ray-sphere intersection for sphere
	\[
		|P - P_c|^2 = r^2
	\]
	is solving for $t$ in
	\begin{align*}
		a t^2 + bt + c &= 0\\
		a &= d \cdot d\\
		b &= 2 (P_0 - P_c) \cdot d\\
		c &= |P_0 - P_c|^2 - r^2
	\end{align*}
	If there are zero solutions, no intersection. If there is a unique solution, barely intersect. If two solutions, through sphere and take smallest $t$
	\item Ray-triangle intersection: two methods
\end{itemize}

Barycentric coordinates (triangle)
\begin{itemize}
	\item Any point $P$ inside a triangle can be written as
	\[
		P = uA + vB + wC, u+v+w=1, u>0, v>0, w>0
	\]
	where $u,v,w$ are barycentric coordinates of $P$ on triangle
	\item Used for interpolation using vertex values and point in triangle tests
	\item Imagine $u$ as the signed triangle area opposite to $A$, and $v$ as triangle area opposite to $B$, etc. Therefore to check intersection, compute $u,v,w$ and if any is negative then $P$ is outside the triangle. (Alternatively, sum the abs coordinates and check if not $1$)
	\item To compute $u,v,w$
	\begin{align*}
		u &= \frac{BCP}{ABC}\\
		v &= \frac{CAP}{ABC}\\
		w &= \frac{ABP}{ABC}
	\end{align*}
	where the area of the triangle is half the parallelogram area computed using the length of the cross product
\end{itemize}

Ray-triangle intersection (method one)
\begin{itemize}
	\item Do a ray plane intersection where plane is 
	\[
		n \cdot (P-A) = 0
	\]
	using the triangle point $A$ and normal $n$. Get intersection point $P$
	\item Check if $P$ is inside the triangle using barycentric coordinates.
\end{itemize}

Ray-triangle intersection (method 2)
\begin{itemize}
	\item Solve a 2d linear system for $u,v$ constraining $w$
	\item Not covered, just copy lecture slide formulas lmao
\end{itemize}

\subsection{Ray traverse}
Traverse after ray hits object
\begin{itemize}
	\item Shadow ray: cast towards light source to check occulusion (occlusion is when some object blocks point from light). If blocked, no color. Otherwise, compute direct illumination from light
	\item Reflection ray: cast in reflection direction using view ray and surface normal at hit point
	\[
		v - 2(v\cdot n) \cdot n
	\]
	\item Refraction ray: cast through surface using snell's law. Simulates glass or water. Use
	\begin{align*}
		\eta = \frac{n_i}{n_t} &= \frac{\sin \theta_t}{\sin \theta_i}\\
		\omega_t &= \eta \omega_i + n(\eta \cos \theta_i - \sqrt{1 - \eta^2 (1-\cos^2 \theta_i)})\\
		\cos \theta_i &= -\omega_i \cdot n
	\end{align*}
	\item All ray derivations from lecture slides
\end{itemize}

\subsection{Shading}
Shading
\begin{itemize}
	\item $C_a$ is ambient
	\item $C_{d,l}$ is the diffuse from light $l$
	\item $C_{s, l}$ is the specular from light $l$
	\item $S_l$ is a boolean variable dictating shadowness
\end{itemize}

Phong illumination model
\[
	C = C_a + \sum_l (C_{d,l} + C_{s,l})
\]

Whitted
\[
	C = C_a + \sum_l S_l (C_{d,l} + C_{s,l}) + k_r C_r + k_t C_t
\]

\subsection{Blending}
Blending
\begin{itemize}
	\item Accumulates the color information from both direct lighting and indirect lighting into a single pixel color
\end{itemize}

\subsection{Computational Cost}
Cost
\begin{itemize}
	\item Rasterization is $O(N_o N_f)$ where $N_o$ is num objects and $N_f$ is pixels per object
	\item Raytracing is $O(N_o N_p)$ where $N_p$ is number of pixels to sample
	\item BVH can accelerate raytracing to $O(N_p \log N_o)$, since rendering time depends on the number of ray-object intersection tests. Tradeoffs: time to build data-structure, dynamic scenes, efficient updates
\end{itemize}

\subsection{Distributed Raytracing}
Distributed/stochastic ray tracing
\begin{itemize}
	\item Developed in 1984
	\item Trace multiple perturbed rays per pixel
	\item Anti-alising by jittering rays within pixel to smooth edges
	\item Soft shadows by sampling shadow rays on area light sources
	\item Depth of field by custom ray distribution from camera over lens aperture
	\item Motion blur by sampling rays at different times during shutter
\end{itemize}

Samples and noise
\begin{itemize}
	\item Can use efficient sampling methods, monte carlo integration, and denoising algorithms to reduce noise in distributed raytracing
	\item CNN can be used to denoise low sample pixels
\end{itemize}


\section{Geometry Transformation}
Column major form
\begin{itemize}
	\item Vectors are column matrices
	\item Matrix elements are indexed by row then column
	\item Multiplication is done right to left
\end{itemize}

Transformation usages
\begin{itemize}
	\item Convenient coordinate systems: object, world, camera, screen
	\item Hierarhcical modelling for reuse
	\item Forward and inverse kinematics
	\item Control projection and rigid body animations
\end{itemize}

\subsection{Two dimensions}
Translations of $T(t_x, t_y)$ on $p$ is
	\[
		p' = p + t
	\]
	
Rotation of $R(\theta)$ on $p$ is
\[
	p' = \begin{bmatrix}
		\cos \theta & -\sin \theta\\
		\sin \theta & \cos \theta
	\end{bmatrix} p	
\]

\subsection{Three dimensions}
Translation is the same as two dimensions. Properties
\begin{itemize}
	\item Zero identity is zero vector
	\item Additive using vector addition
	\item Commutative
	\item Inverse by negation of vector
\end{itemize}

Rotation has three matrices for rotation around the x/y/z axis. Each matrix is simply the 2d rotation matrix on the two unused axis and one on the diagonal for the axis of rotation. Properties
\begin{itemize}
	\item Zero identity is the Identity matrix
	\item Additive along the same axis
	\item Commutative along the same axis. Not on different axis
	\item Inverse is the transpose
\end{itemize}

OpenGL has methods for translation and rotation.

\subsection{Homogeneous Coordinates}
Insights
\begin{itemize}
	\item In Euclidean space, two parallel lines never intersect
	\item In projective space, two parallel lines can intersect at infinite
\end{itemize}

Homogeneous coordinates
\begin{itemize}
	\item Represents an N-dimensional point using $N+1$ numbers
	\item $(x,y,z,w)$ in homogeneous space is $(x/w, y/w, z/w)$ in euclidean space
	\item $(x,y,z)$ in euclidean space has infinitely many representations in homogeneous coordinates by varying $w$. We typically set $w=1$
	\item If $w=0$, the point is at infinity in the direction of $(x,y,z)$. Therefore it is a vector $(x,y,z)$
	\item Scale invariant. Multiplying all by the same non-zero scalar doesn't change the point
\end{itemize}

Homogenous coordinates pros
\begin{itemize}
	\item Express perspective projection as matrix
	\item Combine rigid transformations (translation, rotation, scaling, projection) in a single 4x4 matrix
	\item Natural way of representing points at infinity (vector)
\end{itemize}

Translation matrix writes the translation on the 4th column. Rotation matrix is unchanged with one at the 4th dimension.

Point vector distinction
\begin{itemize}
	\item Point has a non-zero $w$
	\item Vector has a zero $w$
	\item All point vector operations are simply vector operations
	\item Vector plus vector is a vector
	\item Point plus vector is a point
	\item Point minus point is a vector
	\item Transformation matrices on points and vectors are identical. Translation matrix on vectors are basically identity.
\end{itemize}

\subsection{Affine Transformation}
Transformation types
\begin{itemize}
	\item Euclidean transformation: preverses shape and size. Translation, rotation, reflection
	\item Affine transformation: preserves parallelism, collinearity, and proportions (does not preserve lengths or angles). Euclidean transformation and scaling/shearing.
	\item Perspective transformation is not affine
	\item All are just 4x4 matrices with constraints.
\end{itemize}

Affine transformation
\begin{itemize}
	\item Composition of affine transformations are also affine
	\item For any two triangles, there exists an affine transformation mapping one to the another. This allows us to transfer affine transformations on one mesh to transformations on another mesh (multiply affine transformation between triangles and applied affine transformation)
	\item Affine transformation matrix form in homogeneous coordinates is
	\[
		\begin{bmatrix}
			p'\\
			1
		\end{bmatrix} = \begin{bmatrix}
		A & t\\
		0 & 1
		\end{bmatrix} \begin{bmatrix}
		p\\
		1
		\end{bmatrix}
	\]
	or
	\[
		p' = Ap + t
	\]
	where $A$ is the affine transformation, and $t$ is the translation.
\end{itemize}

Shearing
\begin{itemize}
	\item Sliding an object in a direction parallel to a coordinate axis or plane
	\item Shearing factor determines the amount of sliding
	\item Represented by unsymmetric off-diagonal values
\end{itemize}

Composition
\begin{itemize}
	\item To combine transformations, multiply them into a single matrix
	\item Order matters when combining transformations since matrix mul is not commutative. Typical order: rotation, scaling, translation
	\item Rightmost transformation applied first in matrix form. Lastmost transformation in OpenGL applied first in OpenGL.
\end{itemize}

Inverse transformation
\begin{itemize}
	\item Every affine transformation (except zero) is invertible
	\item The matrix for the inverse transformation is the transformation's matrix $A$ inverse. $A$ is invertible if it is non-singular.
	\[
		p = A^{-1} p' - A^{-1} t
	\]
\end{itemize}

\subsection{Quaternions}

Fixed angle rotations
\begin{itemize}
	\item 4x4 matrix form has the top left 3x3 matrix as rotation matrix
	\[
		\begin{bmatrix}
			R & 0\\
			0 & 1
		\end{bmatrix}
	\]
	However, rotation interpolation not straightforward, and it takes $9$ numbers.
	\item Fixed angle form stores the rotation angles about fixed global axis x/y/z (euler angles pitch yaw roll). The rotations are then composed in a fixed order. 
\end{itemize}

Fixed angle (euler angles) problems
\begin{itemize}
	\item Gimbal lock. A loss of DOF in 3d rotation when two rotation axes are parallel, eliminating one rotation axis.
	\item Interpolating euler angles may not produce a unique result (even if both start and end orientation are identical, and interpolation is linear). This is because there are non-unqiue euler angles for the same orientation. 
\end{itemize}

Quaternions
\begin{itemize}
	\item 4D complex numbers with $i,j,k$ plus a real axis. Defined as
	\[
		(s, v)
	\]
	\item Assocative in multiplication but not commutative
	\item Allows compact representation of quaternions and smooth and unique interpolation between rotation
\end{itemize}

Quaternion rotations
\begin{itemize}
	\item The quaternions $q$ to rotate $\theta$ around $n = (x,y,z)$ is (where $n$ is normalized)
	\[
	q = \cos (\theta/2) + \sin(\theta/2) (xi + yj + zj)
	\]
	Then to rotate a vector $v$ by angle $\theta$ around $n$ is
	\[
	(\_, v') = q (0, v) q^{-1}
	\]
	\item Combining rotations is just quaternion multiplications
	\item Can convert quaternions to rotation matrix forms (for OpenGL MVP on GPU), and from rotation matrices to quaternions for interpolation. Formula on lecture slides
\end{itemize}
	
	
\section{Camera transformations}
Coordinate system
\begin{itemize}
	\item A frame of reference defined by an origin and a set of basis vectors. The basis vectors are orthonormal.
	\item Any vector in the space is a linear combination of vasis vector. The column form of the vector is simply this linear combination weights.
\end{itemize}

Coordinate system pipeline
\begin{itemize}
	\item Object space, world space, camera space, NDC, screen space
	\item Model transform, camera/view transform, projection transform, viewport transform
\end{itemize}

Orthogonal matrices
\begin{itemize}
	\item Columns are orthogonal basis vectors that defines the axes of the frame
	\item A square matrix is orthogonal iff $MM^T = I$, meaning that columns are orthogonal unit vectors
	\item Properties: inverse is transpose, closed under multiplication, determinant is plus minus 1, length preserved, angles preserved
\end{itemize}

Rotation matrix is orthogonal, so its inverse is easy to compute.

\subsection{Model Transform}

Coordinate system transformation (model transform)
\begin{itemize}
	\item The world position for point with local coordinates $p$ where the coordinate frame matrix is $M$ is $Mp$.
	\item To handle translation, the coordinate frame matrix is the position of the origin point of the new coordinate system
	\item To handle rotation, the coordinate frame matrix has columns the basis vectors of the coordinate frame's basis vectors
	\item The inverse of this transformation involves inverting the matrix.
\end{itemize}

\subsection{View Transform}

Pinhole camera
\begin{itemize}
	\item Idealized model of a camera where everything is in sharp focus. Can be replaced with an aperture for realism
	\item Computer generated look
\end{itemize}


View transformation
\begin{itemize}
	\item Convert world coordinates to camera coordinates. Camera view frustum is the area that is visible after projection
	\item Under basis, the matrix is
	\begin{align*}
		C_w &= (O_w, e)\\
		C_c &= (O_c, b)\\
		V &= \begin{bmatrix}
			R(b,e) & O_c - O_w\\
			0 & 1
		\end{bmatrix}
	\end{align*}
	\item The formulas are mostly on the slides
\end{itemize}

\subsection{Projection Transform}
Projection Types
\begin{itemize}
	\item Orthographic projection. Preserves parallel lines, no distortion
	\item Perspective projection. Produces foreshortening, mimics human vision
\end{itemize}

Orthographic projection
\begin{itemize}
	\item Need to map the z values from near/far plane to $[0, 1]$ 
	\item Don't need to touch $w$
	\item For raycasting, cast rays in a fixed direction
\end{itemize}

Perspective projection
\begin{itemize}
	\item Need to map z values from near/far plane too. Also need to scale down $x,y$ by $z$
	\item Uses FOV for the mapping/scaling
	\item For raycasting cast from point to plane
\end{itemize}

Panoramic camera
\begin{itemize}
	\item For raycasting cast from origin to all directions
	\item Usually lighting maps
\end{itemize}

\section{Hierarchical modelling}
Skeleton
\begin{itemize}
	\item Represents hierarchical structure of parts (rigidbodies)
	\item Joints connect parts, different joint types allows different movements (with different model transformations)
	\item Ball-and-socket joints for rotation. Linear actuators for translation
\end{itemize}

Joints
\begin{itemize}
	\item Defines the relative positions between two rigidbodies
	\item Creates a hierarchy of connected rigidbodies to actuator
	\item DoF is the number of independent motions available. \item DoF of a full body is the sum of the DoF of each joint
	\item Each joint's rotation/translation are defined accordining to its local coordinate system
\end{itemize}

Rigidbody model transformation
\begin{itemize}
	\item Assume rigidbodies are in a skeleton
	\item For forward kinematics (object to world), its model transform is the product of joint transform matrices.
	\item If the object already has a world position, we can recompute its position when moving a joint by: undoing the model transformation until the joint, applying new joint transformation, reapplying model transformation after joint
\end{itemize}

Motion capture data
\begin{itemize}
	\item ASF file defines the skeleton structure
	\item AMC file defines the motion
\end{itemize}

\section{Rasterization and Shading Models}
Concepts in the rendering pipeline
\begin{itemize}
	\item Physically based rendering
	\item Computational photography
	\item Learning base reconstruction
	\item 3d display
	\item Foveated rendering stereo image
	\item Perceptio
\end{itemize}

Rasterization vs Ray tracing
\begin{itemize}
	\item Rasterization projects triangles to the screen and fills the pixels. Ray tracing shoots rays from camera, find interactions
	\item Rasterization uses z-buffer to determine visibility of a pixel. Ray tracing uses the nearest ray-object intersection
	\item Rasterization is fast and real time. Ray tracing is computationally heavy
	\item Rasterization used in games. Ray tracing used for photorealistic renders
\end{itemize}

Rasterization pipeline
\begin{itemize}
	\item Data: scene with pure 3d geometry data
	\item Shader: vertex shader is ran per vertex, often doing MVP transformation to screen space scene data
	\item Shader: fancy stuff like primitive assembly, tessellation, geometry shader, which overall outputs a list of triangles
	\item Shader: rasterization converts float triangle vertices to pixels filled
	\item Shader: fragment shader to color each pixel
\end{itemize}

The only programmable shaders are the vertex and fragment shader. Both are ran on the GPU.

Triangle rasterization process
\begin{itemize}
	\item Need to find pixels whose centers are inside the triangle. Note that pixel centers are $(x+0.5, y+0.5)$.
	\item A point is inside the triangle if it is on the same side of all the triangle's edges. This is checked via 2d signed cross products between $(P-P_0) \times (P_1-P_0)$ and it is inside if all three has the same sign.
	\item If it is 3d, check if all cross products are pointing in the same direction (against a plane normal using the dot product)
	\item Aliasing is when there arent enough pixels so there are jagged edges. SSAA (super sampling anti aliasing) improves this by sampling $n\times n$ points per pixel (in a fixed uniform grid), and averaging their fragment shader colors.
	\item AA produces gradient colored edges that are blurrier
\end{itemize}

Painters algorithm
\begin{itemize}
	\item Render objects from back to front, later objects overwrite existing colors
	\item Can have unresolvable depth ordering
\end{itemize}

Z-buffer
\begin{itemize}
	\item Idea: save the current minimum z-value per pixel, and only overwrite if new z-value is smaller
	\item Algorithm: initialize depth buffer to inf, for each coloring, skip if z-value higher than buffer, overwrite pixel and z-value if z-value is lower than buffer
	\item Z-buffering (depth buffering) used in conjunction with rasterization. It has the same shape as the pixel buffer.
\end{itemize}

Phong reflection model
\begin{itemize}
	\item involves: ambient, diffuse, and specular
	\item $L = L_a + L_d + L_s$.
\end{itemize}

Diffuse reflection
\begin{itemize}
	\item Diffuse reflection assumes that light scatters uniformly towards every direction in the semi-sphere. This means that it is independent of view direction
	\item Lambert's cosine law states that the proportion of light received is $\cos \theta = n \cdot w$ where $\theta$ is angle between object normal $n$ and light ray $w$.
	\item Light falloff follows the inverse square law.
	\item The diffuse reflection contribution for light $w$ is therefore
	\[
		L_d = K_d \frac{I}{r^2} (n\cdot w) = K_d \frac{I}{r^2} \max(0, n\cdot w)
	\]
	where $I$ is the light color, and we bound the cosine factor to be positive to fix backface lighting 
\end{itemize}

Specular reflection
\begin{itemize}
	\item Specular reflection assumes that light reflects mostly in the direction as if the surface is a mirror. Depends on the view direction
	\item The smaller the angle between the view direction and reflection direction, the brighter the color
	\item Phong specular contribution is therefore
	\[
		L_s = K_s \frac{I}{r^2} \max(0, v \cdot w_r)^p
	\]
	where $I$ is the light color, $w_r$ is the reflected light vector, $v$ is the view vector, and $p$ is the specular/sharpness term. The larger the specular term, the sharper/concentrated the specular reflection
	\item To compute the reflection vector, use
	\[
		w_r = 2(w \cdot n)n - w
	\]
	since $w+w_r$ is in the direction of the normal, but twice the length of $w$ projected onto $n$.
	\item Blinn-phong specular reflection uses a simplier approximation to phong. Let $h$ be the half vector (average) between $v$ and $w$, $h = \frac{v+w}{|v+w|}$, then the specular contribution is
	\[
		L_s = K_s \frac{I}{r^2} \max(0, v \cdot h)^p
	\]
\end{itemize}

Ambient term
\begin{itemize}
	\item Assumes that there exists some minimum light level
	\item Ambient contribution is
	\[
		L_a = K_a I_a
	\]
\end{itemize}


\section{Shading frequency model}
The three shading frequency models are
\begin{itemize}
	\item Per face (flat shading)
	\item Per vertex (Gouraud shading)
	\item Per pixel (Phong shading)
\end{itemize}

Flat shading
\begin{itemize}
	\item Shade all pixels on each face once using face normal vector for lighting
	\item Not good if desires a smooth surface
	\item Done outside a shader
\end{itemize}

Gouraud shading
\begin{itemize}
	\item Shade each vertex of a triangle using its vertex (averaged) normal
	\item Interpolate using the colors of the three vertices for pixels inside the face
	\item Not perfect shading
	\item Done in vertex shader
\end{itemize}

Phong shading
\begin{itemize}
	\item For each pixel, interpolate the vertex normals on the three vertices for its normal. Use interpolated normal to compute shading for each pixe
	\item Not the Blinn-Phong reflection model
	\item Slowest but most accurate shading model
	\item Done in fragment shader
\end{itemize}

Shading frequency model comparison
\begin{itemize}
	\item As vertex number increases, all three models converge to phong shading
	\item Use phong at low vertices, and flat/gouraud at high vertices
\end{itemize}

Shader program
\begin{itemize}
	\item Operations on a vertex or fragment
	\item GPU will run all in parallel
\end{itemize}

Barycentric interpolation
\begin{itemize}
	\item Interpolation to smooth vertex values across face
	\item Values include: colors, normals, uv, positions, depth, material attributes
	\item To compute the barycentric coordinates, use the cross product area formula between all edges and $P$.
	\item Barycentric coordinates are not invariant under 3d to 2d non-orthographic projection. It is however invariant under homogeneous coordinates, so use 2d + depth to compute 3d barycentric coordinates post-projection
\end{itemize}


\section{Texture mapping}
Texture mapping
\begin{itemize}
	\item Texture colors/information are displayed by substituting $K_d, K_s$ (or even normals) in the shading program per pixel
	\item Texture mapping is the mapping of surfaces in 3d space into the 2d texture space. Each pixel in a 3d object space is uniquely mapped to a point on 2d image space (output) and 2d texture space (texture)
	\item Essentially, each triangle copies a piece of the texture image onto the surface
	\item Assume that the mapping is known
\end{itemize}

Texture coordinates
\begin{itemize}
	\item Every vertex is assigned a 2d $(u,v)$ texture coordinate. $u,v \in [0,1]$. This is then interpolated per pixel and used to sample texture colors.
	\item Some textures are tilable, meaning that tiling the image across space will lead to no seams.
\end{itemize}

Texture mapping algorithm
\begin{itemize}
	\item For each rasterized pixel $(x,y)$
	\item Find texture coordinate $(u,v)$ using 3d barycentric interpolation
	\item Sample from the texture map using uv
	\item Set $K_d$ to sampled color
\end{itemize}

Texture sampling frequency
\begin{itemize}
	\item The frequency is about how frequent (close) pixels are sampled from the textures. There are three:
	\item 1 to 1 mapping is ideal, each screen space pixel is mapped to one texture pixel
	\item magnification, multiple screen pixels mapped to one texture pixel
	\item minification, each screen pixel mapped to multiple texture pixels
	\item Requires separate techniques for magnification and minification sampling
\end{itemize}

Texture sampling, mangification
\begin{itemize}
	\item Nearest sampling samples the nearest pixel on the texture. Looks pixely
	\item Bilinear interpolation. Treat texture pixels as texels, for each uv, sample 4 nearest texels weighted by their center distance to uv. Essentially square barycentric interpolation on texels. 
\end{itemize}

Texture sampling, minification
\begin{itemize}
	\item Creates: Moire, disjoint pixels at distance; aliasing at close. This disconnect is due to farther pixels covering more effective areas on textures, vice versa.
	\item Supersampling. Costly
	\item Mipmap. Trilinear interpolation. Too smooth
	\item Anisotropic filtering. Ripmap. Elliptical weighted average (EWA filtering)
\end{itemize}

Mipmap
\begin{itemize}
	\item Notice that
	\begin{enumerate}
		\item Low texture resolution looks fine at distance. Blurry at near side
		\item High texture resolution looks fine at near side. Aliasing at distance
	\end{enumerate}
	\item Mipmap precompute textures at multiple resolutions/levels into a pyramid, dynamically select the level per pixel.
	\item Each level has dimensions/resolution $R \over {2^N}$
	\item To compute $N$ per pixel, find uv coordinates of nearby pixels (one up, one right), compute distances between uv coordinates. The larger the distance, the more minified/zoomed-out the sampling, the lower the resolution (higher $N$) we should ues, vice versa. Given distance $L$, use
	\[
		N = \log_2(L)
	\]
	\item Rounding $N$ may create obvious switching between texture resolutions. Instead, trilinear interpolation interpolates on each texture (bilinear) as well as between levels.
\end{itemize}

Ripmap
\begin{itemize}
	\item Mipmap is restricted to averaging across axis-aligned squared zones for texture sampling
	\item Ripmap: tile mipmap along diagonal, and stretched resolutions off-diagonal.
	\item Anisotropic means different behavior in different orientations. Ripmap achieves this by using the stretched off-diagonal resolutions.
	\item Simple ansiotropic filtering allows averaging in non-square areas in any orientations
	\item Elliptical weighted average filtering. Averages in an orientated gaussian region around center.
\end{itemize}

Advanced texture mapping applications
\begin{itemize}
	\item Environment maps. Texture information represents reflection shading. Captured using a reflective sphere. Used to match lighting of an object to a different environment
	\item Displacement map. Texture are sampled to offset vertex positions.
	\item Normal map (bump map). Texture represent normal vectors to simulate displacements
	\item Baked shading. Textures represent ambient occlusion
	\item Procedure texture generation
	\item 3d texture rendering using marching cubes
\end{itemize}

\section{Advanced Ray Tracing}
Recall that rasterization cannot handle
\begin{itemize}
	\item Soft shadows (light sources have area)
	\item Glossy reflection (blurred reflections)
	\item Indirect illumination (bouncing lightrays onto indirect surfaces)
\end{itemize}


Whitted style raytracing
\begin{itemize}
	\item Shading contains: local blinn-phong reflectance model (ambient, diffuse, specular), recursive reflection, recursive refraction
	\item No need for z-buffer due to per-pixel shading with ray intersection distance check
	\item High recursion depth displays more nested reflections/refractions. 2 is the minimum for reflection (surface + reflection). 3 is the minimum for refraction (surface + inside + outside)
\end{itemize}

Accelerating ray-triangle intersection
\begin{itemize}
	\item Idea: contain each object/set in a larger bounding box. Ray intersects the object only when it intersects with the bounding box
	\item Axis-aligned bounding box is defined by 3 pairs of planes on each axis. The enclosed area is the AABB
\end{itemize}

AABB intersection
\begin{itemize}
	\item Idea: the ray enters the AABB when it enters all pair of planes; the ray exits the AABB when it exits any pair of planes
	\item Compute the ray intersection with each pair of planes. Denote them as $t_{min}, t_{max}$, corresponding to entry and exit points.
	\item Let $t_{enter} = \max(t_{min})$ and $t_{exit} = \min(t_{max})$ over all 3 plane pairs. Note that all $t > t_{enter}$ could be inside the box, and all $t > t_{exit}$ are outside the box
	\item If $t_{enter} < t_{exit}$, there must be a section of ray inside the box.
	\item Assuming previous, if $t_{exit} \geq 0$, the ray must intersect the ray as it either starts inside or outside the box. Otherwise, the ray would starts outside the box.
	\item Both conditions must sat for intersection
\end{itemize}

BVH
\begin{itemize}
	\item A binary tree structure containing triangles
	\item Nodes are either leafs with faces, or non-leafs with two branches. All nodes have an AABB
	\item To build, recursively split along the longest axis and build child nodes. Terminate when (leaf) node contains fewer than some set number of faces
	\item To intersect, check AABB intersection. If non-leaf, check left and right recursively. If leaf, check all faces
\end{itemize}

\section{Path Tracing}
Whitted style ray tracing problems
\begin{itemize}
	\item Reflection/refraction rays only follow mirror/refraction direction
	\item Only considers direct illumination from light sources
	\item Violates energy conservation laws (point to eye energy of a point should always be lesser than the total light to point energy)
	\item Hard shadows
\end{itemize}

Physically accurate rendering equation
\[
	L_o(\omega_o) = L_e(\omega_o) + \int_\Omega L_i(\omega_i) (n\cdot \omega_i) f_r(\omega_i, \omega_o) d\omega_i
\]
\begin{itemize}
	\item Light emitted to $\omega_o$ is the sum of the emission of the surface and the total illuminated light from all directions
	\item Total illuminated light computed by integrating over all possible light direction on their colors weighted by intensity (cosine/dot term) and BRDF material properties
\end{itemize}

Material properties
\begin{itemize}
	\item Diffuse material tend to reflect light uniformally
	\item Coated material tends to reflect light along mirror direction
	\item Translucent material tends to refract light in the refraction direction
\end{itemize}

Bidirectional Reflectance Distribution function
\begin{itemize}
	\item Describes how light bounces off the surface. Different for each material
	\item $f_r(\omega_i, \omega_o)$ is a number specifying the proportion of light reflected along direction $\omega_o$ from incoming light from direction $\omega_i$
	\item A 4D function. Each vector represented by two polar angles (yaw and pitch angle)
	\item Conservation of energy requires
	\[
		\int_\Omega f_r(\omega_i, \omega_o) d\omega_o \leq 1
	\]
	so that the total reflect light energy is less than incident light energy
	\item Reciproical
	\[
		f_r(\omega_i, \omega_o) = f_r(\omega_o, \omega_i)
	\]
\end{itemize}

BRDF capture
\begin{itemize}
	\item The phong reflectance model technically constructs a BRDF in combining the diffuse and specular terms
	\item Can also be measured irl
\end{itemize}

Path tracing
\begin{itemize}
	\item Rendering algorithm following the rendering equation
	\item Produces true photo-realistic images with: glossy reflections, global illumination, soft shadows
	\item Each sample traces the ray recursively using a randomly sampled reflection direction per surface under the BRDF distribution. Average the pixel color over many samples. This is a monte carlo sampling over the continuous rendering equation
\end{itemize}

Path tracing properties
\begin{itemize}
	\item More samples leads to less noisy renders
	\item Global illumination with higher bounce counts tends to increase overall scene brightness (especially in shadows).
\end{itemize}

\section{Computer Animation}
Hand drawn animation
\begin{itemize}
	\item Skilled animator draws the key poses
	\item Assistant fills in the inbetween frames, tweens
	\item Follows the principles of animation
\end{itemize}

Computer animation
\begin{itemize}
	\item Skilled animator is the editor, making keyframes
	\item Assistant animator is the computer
\end{itemize}

Keyframe animation
\begin{itemize}
	\item Keyframes are data points that defines the important poses. Each stores the exact desired state of the model at a specified itme
	\item Timing refers to where in time keyframes are placed. Timing dictates the speed of the motion
	\item In-betweens are computer generated interpolation. Important distinction between what (angle or endpoints) and how (linear, splines) to interpolate
	\item Ideal interpolation is smooth with user controls
\end{itemize}

Parametric curves
\begin{itemize}
	\item The 3d position of an object under animation is a parametric curve with time parameter $u$
	\item Formula is $Q(u)$, where $u \in [0,1]$
\end{itemize}

Linear interpolation
\begin{itemize}
	\item Between $p_0$ and $p_1$ is
	\[
		Q(u) = (1-u)p_0 + u p_1
	\]
	\item Produces a straightline using weighted average between endpoints.
	\item For piecewise linear interpolation, linear interpolate each segment, and mod time to get the correct segment and interpolation parameter $u$.
\end{itemize}

Spline curves
\begin{itemize}
	\item A piecewise polynomial curve providing smooth and natural animation.
	\item Types: interpolating through points, approximating near points
	\item Order-n spline uses $n+1$ control points
	\item Basic cubic spline is
	\[
		Q(u) = a_0 + a_1u + a_2 u^2 + a_3 u^3
	\]
	with four parameters each determined by four constraints (endpoints and tangents). Sufficient but hard to specify
	\item Basis function cubic spline is
	\[
		Q(u) = B_0(u)P_0 + B_1(u) P_1 + B_2(u)P_2 + B_3(u)P_3
	\]
	where $B_i(u)$ are basis functions that determines the behavior of the spline, and $P_i$ are control points.
\end{itemize}

Cubic bezier spline
\begin{itemize}
	\item Uses Bernstein polynomials. Two endpoints, two control points
	\item Equations from lecture slides
	\item Curve doesn't pass control points
	\item Curve inside the convex hull of control points
	\item Intuitive for design and modeling (CAD)
\end{itemize}

Cubic hermite spline
\begin{itemize}
	\item Equation from lecture slides. Two endpoints, two tangents
	\item Curve passes control points
	\item Tangent can specify the smoothness
\end{itemize}

Catmull-rom spline
\begin{itemize}
	\item Special case of cubic hermite
	\item Tangent automatically calculated by nearby control points.
	\item Curve passes all control points
	\item Good for animation and path design
	\item Each segment is defined by 4 points. Two endpoints and two points immediately before and after to derive tangents
\end{itemize}

Continuity
\begin{itemize}
	\item Linear interpolation is $C_0$ continuous
	\item Catmull-rom is $C_1$ continuous
\end{itemize}

Arc length parameterization
\begin{itemize}
	\item Equally spaced parameter values do not produce equally spaced points.
	\item Leads to non-uniform motion by increasing $u$
	\item Arc-length is a functiono mapping $u$ to true distance. Reparameterize to a new arclength parameter so uniform speed along the curve
\end{itemize}

Chord length parameterization
\begin{itemize}
	\item Exact arc-length too expensive
	\item Approximate using chord length between uniform samples of $u$
	\item Construct a table mapping equal-spaced $u$ to chord length (prefix sum to approximate arc length)
	\item To find arclength at $u'$, divide $u$ by sampled interval, linearly interpolate chord length between two closest sampled point of $u$ next to $u'$.
	\item Similar calculations to invert the arclength table (mapping arclength to $u$)
	\item Reparameterize curve to $Q(S^{-1}(s))$ so equal $s$ samples are equal-distance (and constant speed) apart
\end{itemize}

Rotation interpolation
\begin{itemize}
	\item Interpolating rotation matrices doesn't work since it is not a valid rotation matrix. Also redundancies in matrix coefficients and non-unique solution
	\item Interpolating quaternions perform better than interpolating euler angles
	\item SLERP produces smooth constant angular speed rotation along spherical angles. Formula on lecture slides.
	\item Chaining SLERP is only $C^0$, so visible kinks at keyframes
	\item SQUAD is quaternion splines. Takes two endpoints and two control quaternions. Formula on lecture slide uses SLERP.
	\item Chaining SQUAD is $C^1$ continuous (constant angular velocity). 
\end{itemize}

\section{Skinning/Rigging}
Rigidbody animation
\begin{itemize}
	\item Objects moving as a whole piece in: translation, rotation, scaling
	\item No internal deformation.
\end{itemize}

Non-rigidbody animation
\begin{itemize}
	\item Mesh deformation
	\item Physics based
\end{itemize}

Skeletal animation
\begin{itemize}
	\item Skeleton is a hierarchical structure containing bones and joints
	\item Bones have their local transforms
	\item Mesh is bound to the skeleton. As skeleton/bones move, meshes should follow
\end{itemize}

Human animation
\begin{itemize}
	\item Human articulated by skeleton (rigid motion) and soft tissue mesh (non-rigid motion)
	\item Requires anatonmical interactions between rigid and non-rigid materials
\end{itemize}

Skinning
\begin{itemize}
	\item The process of binding a mesh to a skeleton
	\item Vertex position influenced by one or more bones
	\item Methods: linear blend skinning, dual quaternion skinning, pose space deformation
\end{itemize}

Linear blend skinning
\begin{itemize}
	\item Most commonly used
	\item Vertex position is weighted average of translation of nearby/influencing bones
	\item Pros: efficient gpu friendly, well for games and real-time rendering
	\item Cons: collapsing joints, candy-wrapper twists
\end{itemize}

Dual quaternion skinning
\begin{itemize}
	\item Alternative LBS method to produce accurate rotations (no candy-wrapper twist)
	\item The rotation quaternions are averaged first before application
	\item Pros: still widely supported, more expensive to compute
\end{itemize}

Skinning weights
\begin{itemize}
	\item Each vertex influenced by 1-4 bones
	\item Either manually created by painting, or automatically by distance
	\item Weights must sum to 1
\end{itemize}

Pose space deformation
\begin{itemize}
	\item Corrective meethods applied after skinning
	\item Use example true poses to create vertex corrections.
	\item Interpolate vertex corrections based on current pose and sample pose information
	\item Use for high-quality characters
\end{itemize}


Body vs face animation
\begin{itemize}
	\item Body is driven by skeleton and skinning
	\item Face uses blendshapes (morph targets) that are predefined facial poses. Allows lip-syncing and showing expressions 
\end{itemize}

Blendshapes
\begin{itemize}
	\item Are predefined facial poses
	\item Can blend between sample shapes using weighted average between blendshapes. Or blend using shape parameters (essentially normalized blendshape weights against a base mean shape)
	\item Blending using shape parameters is preferred
	\item Formulas on lecture slides
\end{itemize}

Blendshape pros and cons
\begin{itemize}
	\item Pros: can handle deformation independent of skeletal movements; intuitive interpolation between expressions; simple and fast
	\item Cons: requires all shapes to have identical topology; limited to subspace of key shapes; cannot control detailed/specific deformation
\end{itemize}

\section{Input mechanism}
Covers the ways that a CG application can take inputs actions from users.

Interactive computer graphics uses
\begin{itemize}
	\item Video games
	\item Simulations
	\item CAD
	\item Data visualizations
	\item AR/VR
\end{itemize}

Input mechanisms
\begin{itemize}
	\item Keyboard
	\item Mouse
	\item Controller
	\item Eye tracking
	\item Touch
\end{itemize}

Keyboards
\begin{itemize}
	\item Influenced by typewriters
	\item Comes in many forms and layouts today not just qwerty and ansi layout
	\item Try to leverage people's mental models (or past experiences) when creating control schemas (ctrl for crouch, space for jump)
	\item Avoid right side of keyboard if the mouse is used
	\item Don't assume non-standard setups
\end{itemize}

Mouse
\begin{itemize}
	\item Don't expect people to have more than the standard number of mouse buttons
	\item Use mouse for 3d viewing camera controls
	\item If a lot of keybinds, allow user to rebind their extra mouse buttons to them. Opt-in instead of opt-out here.
\end{itemize}

Controller
\begin{itemize}
	\item Assume standard PS5 and Xbox one controller scheme
\end{itemize}

Eyetracking
\begin{itemize}
	\item Uses laser beam, device placed under screen
	\item Allows hands-free control
	\item Hard to use/make it well
	\item Nowdays used for VR
\end{itemize}

Touch
\begin{itemize}
	\item Faster operations compared to keyboards
	\item Easy to use, intuitive and requires less coordinations
	\item Accessibility benefits, helps users with physical limitations in interaction
	\item Device size, requires smaller screens 
\end{itemize}

GUI
\begin{itemize}
	\item The on-screen UI consists of: information display elements, interactable components
	\item Fitts law. Models how difficult it is for someone to hit a target
	\[
		ID = \log_2(\frac{2D}{W})
	\]
	where $ID$ is the index of difficulty, $D$ is distance to target center, $W$ is the width of the target.
	\[
		MT = a + b ID
	\]
	where $MT$ is the movement time to hit the target, and $a,b$ are empirical constants depending on device.
	\[
		TP = \frac{ID}{MT}
	\]
	where $TP$ is the throughput (index of performance), the amount of information that the user can contribute
	\item Therefore make targets larger and closer to maximize the user contribution
\end{itemize}

Infinite edges
\begin{itemize}
	\item For targets on the edge, model it as a target with infinite width/height
	\item For targets on corners, model it as targets with infinite width and height
	\item Put the most frequently pressed buttons on edge or corners
\end{itemize}

\section{Human cognition}
Covers how humans react to input/information of a CG application.

Flawed humans
\begin{itemize}
	\item Humans make mistakes, even if the developers think otherwise
	\item Mistakes due to: inexperience, ambiguity, lack of symbol mapping, confusing symbol mappijng
	\item Requires special design choices to prevent such mistakes from happening (color coding, hardware changes, symbol placement)
\end{itemize}

Attention
\begin{itemize}
	\item User attenion may need to be grabbed at the right moements
	\item Design primitives: color, font, weight, sound, light
	\item Cues and highlights: highlight current location and intended future path
\end{itemize}

Progressive disclosure
\begin{itemize}
	\item a hierarchy of details that are progressively shown if the user intends to
\end{itemize}


Color palette
\begin{itemize}
	\item Don't use eye burning palettes. Use a simple color palette
	\item Most colors are distractful. Most should be uninteresting
	\item Eye fatigue: blue and red, blue and yellow, green text on red (vice versa)
	\item Use blue in large areas not thin lines
	\item Careful about colorblindness: red and green in center
	\item Black, white, and yellow in periphery
	\item Contrast and saturation, choose colors that maximizes them
\end{itemize}

Shapes and perception
\begin{itemize}
	\item Use shapes to make objects bigger
	\item Use edge rendering in place of full shapes
\end{itemize}

Aesthetics
\begin{itemize}
	\item Aesthetics are cruical to how players perceive the game
	\item Understanding human perception helps to design games that feel right
\end{itemize}

Mental models and memory
\begin{itemize}
	\item Use real life symbols to denote actions similar to real life actions
	\item Short term memory: rapid access but limited capacity
	\item Long term memory: slow access but huge capacity
	\item Sensory memory, buffer for stimuli received through senses, continuously overwritten
\end{itemize}

Learning curve
\begin{itemize}
	\item Low number of features is boring
	\item Ok number of features is fun
	\item A lot of features makes it complicated
	\item Good game design allows the user to learn by doing. Avoid requiring a manual or FAQ page.
	\item Change your game to match the players
\end{itemize}

\section{Evaluation Techniques}

Human-centered design
\begin{itemize}
	\item Iterate repeatly with your users
	\item The user should be the focus. Modify your app to fit the users
	\item Typically done multiple times to check perception.
\end{itemize}

User experience evaluation
\begin{itemize}
	\item User Testing: who is the end user, how they will use my app
	\item Usability Testing: can the end user use my app
	\item Basically: how users are using the app, can users use the app expectedly
\end{itemize}

\subsection{Observational method}
Observational method
\begin{itemize}
	\item Watch users and see how they do in game. See where things breakdown
	\item Involves: think aloud, cooperative evaluation, post-task walkthrough
	\item Ideally perform all methods.
\end{itemize}


Think aloud
\begin{itemize}
	\item Participant plays the game. When they play, ask them to vocalize their thinking
	\item Take notes on their behavior/thoughts. No interfering
	\item What are they doing? Why? What they will do next? What do you expect to happen?
	\item Pros: simple and insightful
	\item Cons: subjective, awkard, might change how they play
\end{itemize}

Cooperative evaluation
\begin{itemize}
	\item Variation of think aloud, where user can interact with the experiment by asking questions. Experimenter can also ask questions
	\item Pros: much more relaxed, break up awkward silence, user won't get stuck, encourage to critize software
	\item Cons: overlap and hide problems, less rigorous
\end{itemize}

Post-task walkthrough
\begin{itemize}
	\item Talk to players afterwards after walking them play
	\item Ask users about specific things they have done
	\item To help with memory: show recording of gameplay, make task small
	\item Pros: avoid interruptions, make playing more natural
	\item Cons: they might forget stuff, post-hoc interpretation may be different to in-the-moment 
\end{itemize}

Capturing observations. Can have multiple methods done by multiple people.
\begin{itemize}
	\item Pen-paper
	\item Audio
	\item Video
	\item Computer Logging
\end{itemize}

\subsection{Querying technique}
Querying technique
\begin{itemize}
	\item Specifically ask the users about their experience
	\item Involves: interviews, questionaire
\end{itemize}

Interviews
\begin{itemize}
	\item Question users one-on-one based on prepared questions. After they've played the game.
	\item No specific focus on one stretch of gameplay compared to post-task walkthrough, Focuses on the general game experience
	\item Pros: cheap, varying detail, elicit user views
	\item Cons: time consuming to analyse, hard to balance depth and breadth
\end{itemize}

Interview questions
\begin{itemize}
	\item Question types: closed questions with predetermined answer format, open questions with no format
	\item Closed questions are easier to analyse but lack details. Should avoid
	\item Avoid: long questions, leading questions, compound questions, jargon
\end{itemize}

Interview styles
\begin{itemize}
	\item Structured: verbal questionnaire, lacks depth
	\item Semi-structured: more difficult to do, requires background knowledge, can deviate at interesting answers
	\item Open-ended: having only a starting question, requires no knowledge, hardest to administer and analyse
\end{itemize}

Questionnaires
\begin{itemize}
	\item Fixed, written questions given to users
	\item Pros: easy to administer and analyse, easy to compare
	\item Cons: lacks depth
\end{itemize}

Custom questionnaire
\begin{itemize}
	\item Design the questions to investigate specific topics
	\item Questions: open-ended, scalar, multichoice, ranked
\end{itemize}

Standard scales
\begin{itemize}
	\item Created by researchers to establish norms
	\item Plenty each targeting different aspects. Not tailored to the specific evaluation needs, but can make it easy to compare results between people.
	\item Allows computing a standardized score that can be used under a threshold criteria
	\item Types: post-study questionnaries, questionnaires for website, experimental comparison of post-study questionnaire, etc (lecture slides)
\end{itemize}

Questionnaire process
\begin{itemize}
	\item Decide what to measure
	\item Find standardized questionnaire and create your own
	\item Deploy game and collect data
	\item Analyze data
\end{itemize}

\subsection{Physiological measures}
Eye tracking
\begin{itemize}
	\item What the player is looking at, at a particular scene
	\item Gives insight into cognitive process
	\item Types of eye movements: fixation (focusing), saccades (rapid movement), smooth pursuit (following)
\end{itemize}

Other measurements
\begin{itemize}
	\item Heart rate
	\item Blood pressure
	\item Muscle activity
	\item Brain activity
	\item Sweat glands
	\item Hard to interpret, but provide some insight into the emotional state of user
\end{itemize}

\subsection{Receiving feedback}
Expected to use user evaluation results to improve your game. Improvements must be explained in your report.

Your audience is good at recognizing problems but bad at solving them.

Cursed problems are unsolvable design problems, rooted in a conflict between player promises (what each player think the game should be).
\begin{itemize}
	\item Focus on mastery vs focus on winning
	\item Safety vs addiction
	\item May only be discovered using user evaluation
\end{itemize}


\section{VR}
Reality virtuality (RV) continuum
\begin{itemize}
	\item A spectum from: real environment, augmented reality, augmented virtuality, virtual environment
	\item Real environment is real life. Virtual environment are all abstract. The inbetweens are a mixture between reality and virtuality
	\item VR, near the virtual environment
	\item MR, the middle
	\item XR, all except real life
\end{itemize}

Virtual reality
\begin{itemize}
	\item Fully immersive and interactive CG environment
	\item Immersion is the perception of being physically present in a non-phyiscal world
	\item Presence is the sense of being there
\end{itemize}

VR history
\begin{itemize}
	\item Sensorama
	\item Sword of damocles
	\item Army prototypes: super cockpit, vecta
	\item Cave systems
	\item Nintendo virtualboy
	\item Oculus rift
	\item Facebook acquires oculus
	\item Google cardboard
	\item Tethered headsets
\end{itemize}


VR applications
\begin{itemize}
	\item Gaming
	\item Training and Education
	\item Design and Architecture
	\item Social VR
\end{itemize}

Steoreoscopy
\begin{itemize}
	\item A method to make 3d images for VR
	\item Virtually, two cameras offset by the interpupillary distance rendering the scene
	\item IRL, two eyes seeing two different (slightly shifted) projections from the virtual cameras
\end{itemize}

Optics
\begin{itemize}
	\item Lens are needed to simulate distance with a closeup display
	\item Different people have different eyes/distance, so adjustments mechanisms are required
\end{itemize}

Head and controller tracking
\begin{itemize}
	\item IMU to track rotation
	\item Cameras to detect controller movements and gesture detection
\end{itemize}

Body tracking
\begin{itemize}
	\item External sensors: kinect
	\item Inside-out tracking: meta-quest
	\item Market-based tracking
	\item Wearables
	\item Ai based tracking
\end{itemize}

\subsection{VR technical problems}
VR open problems
\begin{itemize}
	\item Haptics: for real immersion, touching things is needed. Hard to provide haptics, common methods are: passive haptics, active haptics, mid-air haptics, haptics retargeting. Always a tradeoff between accuracy and diversity
	\item Space limitation: cannot navigate large virtual worlds while confined to a small physical space. Locomotion techniques: continuous input (joystick), teleportation, walking in place, treadmill, redirected walking
\end{itemize}

Real world awareness
\begin{itemize}
	\item VR makes people lose real world awareness, can damage IRL things
\end{itemize}

Motion sickness
\begin{itemize}
	\item Cybersickness is a known problem
	\item Some people are more prone to it than others
	\item Getting better with improvements in latency and precision
\end{itemize}

Gorilla arm effect
\begin{itemize}
	\item Tiring to holdup arms all the time; full body gestures can cause fatigue and shoulder pain
	\item Consumed endurance model suggests fatigue after just 90 seconds
	\item Gun slinging proposed as a solution, but removes hand-display feedback which is bad
\end{itemize}

\subsection{VR social problems}
Social problems
\begin{itemize}
	\item Social acceptability
	\item Black mirror effect (VR making people anti-social)
	\item Planetary boundaries (climate effects)
\end{itemize}


\section{Mixed/Augmented Reality}
MR
\begin{itemize}
	\item An environment that presents the real world and virtual world objects together
	\item AR combines real and virtual and is interactive in real time
\end{itemize}

MR technologies
\begin{itemize}
	\item Headset
	\item Projection mapping
	\item Mobile AR
\end{itemize}

AR 2.0
\begin{itemize}
	\item Mobile based vision, with virtual objects correctly situated in the real world
	\item People create and share geolocated information with each other
\end{itemize}

Optical see through
\begin{itemize}
	\item User see the real world directly through transparent lenses
	\item Digital content projected onto lenses
	\item Waveguides used to merge virtual content with irl content
\end{itemize}

Video see through
\begin{itemize}
	\item Real world view captured by cameras on the headset and displayed on screens
	\item Virtual content painted into video feed
\end{itemize}

MR requirements
\begin{itemize}
	\item Occulusion (layering), spatial awareness (shapes), physics
	\item SLAM algorithm processes data from IMUs (camera feed mostly), and repeatedly: build environment map, locate device in environment.
\end{itemize}

SLAM
\begin{itemize}
	\item Since objects in space have a spatial relationship with each other, sensor data can get a good probability dist on where positions are. Many algorithms can be used to calculate the probability distributions
	\item Camera feed can identify unique space features (feature extraction), which are stored as descriptors along with their neighborhood information.
	\item For SLAM, the most data, the higher the precision. But some things are hard to distinguish (white walls, transparent objects, reflective surface)
	\item Once enough feature points are collected, can connect the dots and create a mesh for the real world
\end{itemize}

Implementation
\begin{itemize}
	\item Sources on lecture slide
\end{itemize}

\subsection{AR technical problems}
Geo-referencing
\begin{itemize}
	\item SLAM generates the local environment knowledge, but cannot share that underestanding
	\item Azure spatial anchors may solve it
	\item Marker based AR mitgate this easily, but less flexible
\end{itemize}

Compositing
\begin{itemize}
	\item Compositing is about bringing the visual elements together so that virutal objects looks belonging in irl
	\item Includes occulusion, lighting, shading, and coloring/texturing
\end{itemize}

Plus all the technical problems from VR

\subsection{AR societal problems}
Same as VR

\section{Speaker Lecture}


\end{document}